---
title: "Lab5"
author: "Charlie curtin"
date: "2023-02-07"
output: html_document
---

This week's lab is a musical lab. You'll be requesting data from the Spotify API and using it to build k-nearest neighbor and decision tree models.

In order to use the Spotify API you must have a Spotify account. If you don't have one, sign up for a free one here: <https://www.spotify.com/us/signup>

Once you have an account, go to Spotify for developers (<https://developer.spotify.com/>) and log in. Click the green "Create a Client ID" button to fill out the form to create an app create an app so you can access the API.

On your developer dashboard page, click on the new app you just created. Go to Settings -\> Basic Information and you will find your Client ID . Click "View client secret" to access your secondary Client ID. Scroll down to Redirect URIs and enter: <http://localhost:1410/>

You have two options for completing this lab.

**Option 1**: **Classify by users**. Build models that predict whether a given song will be in your collection vs. a partner in class. This requires that you were already a Spotify user so you have enough data to work with. You will download your data from the Spotify API and then exchange with another member of class.

**Option 2**: **Classify by genres**. Build models that predict which genre a song belongs to. This will use a pre-existing Spotify dataset available from Kaggle.com (<https://www.kaggle.com/datasets/mrmorj/dataset-of-songs-in-spotify>)

```{r, message = FALSE, warning = FALSE}
library(spotifyr) #API interaction
library(tidyverse)
library(tidymodels)
library(here)
library(rpart)
library(rpart.plot)
library(caret)
library(baguette)
```

Client ID and Client Secret are required to create and access token that is required to interact with the API. You can set them as system values so we don't have to do provide them each time.

```{r access_API, include = FALSE, eval = FALSE}
authorization_code <- get_spotify_authorization_code(scope = scopes()[c(1:19)]) #sets an authorization code that you'll need to provide for certain get_ functions via my_tracks <- get_my_saved_tracks(authorization = authorization_code)

access_token <- get_spotify_access_token() # takes ID and SECRET, sends to Spotify and receives an access token
```

**Option 1: Data Preparation**

You can use get_my_saved_tracks() to request all your liked tracks. It would be good if you had at least 150-200 liked tracks so the model has enough data to work with. If you don't have enough liked tracks, you can instead use get_my_recently_played(), and in that case grab at least 500 recently played tracks if you can.

```{r, include = FALSE, eval = FALSE}
# get top 50 tracks
top_songs <- spotifyr::get_my_top_artists_or_tracks(type = "tracks", 
                                       authorization = authorization_code, 
                                       limit = 50,
                                       time_range = "long_term")

# read in partner's music
op_music <- read_csv(here("labs", "data", "spotify_audio_features_op.csv"))
```

```{r, include = FALSE, eval = FALSE}
# create empty dataframe to store songs
charlie_music <- data.frame()

# retrieve music
for (i in seq(0, 280, 20)) {
  music <- get_my_saved_tracks(authorization = authorization_code,
                               offset = i) # get music
  charlie_music <- rbind(charlie_music, music) 
}

# retrieve audio features
features1 <- get_track_audio_features(charlie_music$track.id[1:100])
features2 <- get_track_audio_features(charlie_music$track.id[101:200])
features3 <- get_track_audio_features(charlie_music$track.id[201:300])

charlie_audio_features <- rbind(features1, features2, features3) %>% 
  bind_cols(charlie_music$track.name)

write_csv(charlie_audio_features, here("labs", "data", "charlie_audio_features.csv"))
```

The Spotify API returns a dataframe of tracks and associated attributes. However, it will only return up to 50 (or 20) tracks at a time, so you will have to make multiple requests. Use a function to combine all your requests in one call.

Once you have your tracks, familiarize yourself with this initial dataframe. You'll need to request some additional information for the analysis. If you give the API a list of track IDs using get_track_audio_features(), it will return an audio features dataframe of all the tracks and some attributes of them.

These track audio features are the predictors we are interested in, but this dataframe doesn't have the actual names of the tracks. Append the 'track.name' column from your favorite tracks database.

Find a class mate whose data you would like to use. Add your partner's data to your dataset. Create a new column that will contain the outcome variable that you will try to predict. This variable should contain two values that represent if the track came from your data set or your partner's.


**Option 2: Data preparation**

Download the Spotify dataset from <https://www.kaggle.com/datasets/mrmorj/dataset-of-songs-in-spotify>

Inspect the data. Choose two genres you'd like to use for the classification task. Filter down the data to include only the tracks of that genre.

###Data Exploration (both options)

Let's take a look at your data. Do some exploratory summary stats and visualization.

For example: What are the most danceable tracks in your dataset? What are some differences in the data between users (Option 1) or genres (Option 2)?

```{r, message = FALSE, warning = FALSE}
## data cleaning and wrangling -----
# read in Charlie and Oksana audio features and create binary variable 1 (charlie's music) or 0 (oksana's music)
charlie <- read_csv(here("labs", "data", "charlie_audio_features.csv")) %>% 
  mutate(owner = 1) %>% 
  rename("track_name" = "...19")

oksana <- read_csv(here("labs", "data", "spotify_audio_features_op.csv")) %>% 
  mutate(owner = 0) 

# bind the dataframes together, select variables of interest
combined_music <- rbind(charlie, oksana) %>% 
  select(-c(type, id, uri, track_href, analysis_url, track_name)) %>% 
  mutate(owner = as.factor(owner))

## exploratory visualizations -----
```


### **Modeling**

Create competing models that predict whether a track belongs to:

Option 1. you or your partner's collection

Option 2. genre 1 or genre 2

You will eventually create four final candidate models:

#### Preprocessing

```{r, message = FALSE, warning = FALSE}
set.seed(389)

# split data into training and testing set
music_split <- initial_split(combined_music)
music_train <- training(music_split)
music_test <- testing(music_split)

# specify our recipe
recipe <- recipe(owner ~ ., data = music_train) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  prep()

```

#### K-nearest neighbor

```{r}
# define a nearest neighbors model set to tune the number of neighbors
knn_tune <- nearest_neighbor(neighbors = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn")

# set workflow with our recipe
workflow_tuned <- workflow() %>% 
  add_model(knn_tune) %>% 
  add_recipe(recipe)

# use 5 folds cv
cv_folds <- vfold_cv(music_train, v = 5)

# use a tune grid to create different models
res_tune <- workflow_tuned %>% 
  tune_grid(
    resamples = cv_folds,
    grid = 8
  )

# select best model from the tuned results, 
knn_best <- finalize_workflow(workflow_tuned,
                              select_best(res_tune, metric = "roc_auc"))

# fit our best model to our training data
knn_best_fit <- fit(knn_best, music_train)

# get predictions using our test set and bind to the dataset
knn_predict <- predict(knn_best_fit, music_test) %>% 
  bind_cols(music_test)

# assess accuracy of our model
accuracy(knn_predict,
         truth = owner,
         estimate = .pred_class)

```

2.  decision tree (Week 5)

```{r}
# specify a decision tree and set to tune
dt_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

# set tune grid 
dt_grid <- grid_regular(cost_complexity(), 
                          tree_depth(), 
                          min_n(), 
                          levels = 5)

# set our workflow with our recipe and decision tree tune specifiction
dt_workflow <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(dt_spec)

# use cross folds validation to tune our models
doParallel::registerDoParallel()

dt_res <- tune_grid(
  dt_workflow,
  resamples = cv_folds,
  grid = dt_grid,
  metrics = metric_set(accuracy)
)

# find our best model
dt_final <- finalize_workflow(dt_workflow,
                              select_best(dt_res))

# fit our best model
dt_best <- last_fit(dt_final,
                    music_split)

# show our best model testing accuracy
dt_best$.metrics
```

3.  bagged tree (Week 6)
    -   bag_tree()
    -   Use the "times =" argument when setting the engine during model specification to specify the number of trees. The rule of thumb is that 50-500 trees is usually sufficient. The bottom of that range should be sufficient here.

```{r}
# specify a bagged tree to tune
bag_spec <- bag_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>% 
  set_engine("rpart",
             times = 75) %>% 
  set_mode("classification")

# set tune grid
bag_grid <- grid_regular(cost_complexity(),
                         tree_depth(),
                         min_n(),
                         levels = 5)

# specify our tuned workflow
bag_wf_tune <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(bag_spec)

# use k-folds cv to tune our model
doParallel::registerDoParallel()

bag_res <- tune_grid(
  bag_wf_tune,
  resamples = cv_folds,
  grid = bag_grid,
  metrics = metric_set(accuracy)
)

# find the best model
bag_tree_final <- finalize_workflow(bag_wf_tune,
                                    select_best(bag_res))

# fit our best model to our data
bag_tree_fit <- last_fit(bag_tree_final,
                         music_split)

# view the accuracy of our model
bag_tree_fit$.metrics
```

4.  random forest (Week 6)
    -   rand_forest()
    -   m_try() is the new hyperparameter of interest for this type of model. Make sure to include it in your tuning process

```{r}
# specify our random forest model, set to tune hyperparameters
rf_spec <- rand_forest(mtry = tune(),
                       trees = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# create a workflow with our recipe
rf_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(rf_spec)

# use k-folds cv to tune our model
rf_tune <- rf_wf %>% 
  tune_grid(
    resamples = cv_folds,
    metrics = metric_set(roc_auc),
    grid = 10
  )

# select our best model
rf_final <- finalize_workflow(rf_wf,
                              select_best(rf_tune))

# fit best model to our data
rf_best_fit <- fit(rf_final, music_train)

# fit predictions on our testing data
rf_preds <- predict(rf_best_fit, music_test) %>% 
  bind_cols(music_test)

# assess accuracy of our random forest model
accuracy(rf_preds, 
         truth = owner,
         estimate = .pred_class)
```


Go through the modeling process for each model:

Preprocessing. You can use the same recipe for all the models you create.

Resampling. Make sure to use appropriate resampling to select the best version created by each algorithm.

Tuning. Find the best values for each hyperparameter (within a reasonable range).

Compare the performance of the four final models you have created.

Use appropriate performance evaluation metric(s) for this classification task. A table would be a good way to display your comparison. Use at least one visualization illustrating your model results.
