---
title: "Curtin_Lab3"
author: "Charlie Curtin"
date: 01-30-2024
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(glmnet)
```

## Lab 3: Predicting the age of abalone

Abalones are marine snails. Their flesh is widely considered to be a
desirable food, and is consumed raw or cooked by a variety of cultures.
The age of abalone is determined by cutting the shell through the cone,
staining it, and counting the number of rings through a microscope -- a
boring and time-consuming task. Other measurements, which are easier to
obtain, are used to predict the age.

The data set provided includes variables related to the sex, physical
dimensions of the shell, and various weight measurements, along with the
number of rings in the shell. Number of rings is the stand-in here for
age.

### Data Exploration

Pull the abalone data from Github and take a look at it.

```{r data}
# read in the abalone data
abdat <- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/abalone-data.csv")

# view a glimpse of the dataframe
glimpse(abdat)
```

### Data Splitting

-   ***Question 1***. Split the data into training and test sets. Use a
    70/30 training/test split.

We'll follow our text book's lead and use the caret package in our
approach to this task. We will use the glmnet package in order to
perform ridge regression and the lasso. The main function in this
package is glmnet(), which can be used to fit ridge regression models,
lasso models, and more. In particular, we must pass in an x matrix of
predictors as well as a y outcome vector , and we do not use the yâˆ¼x
syntax.

```{r}
# set seed
set.seed(123)

# split data into training and test (70/30 split)
split <- initial_split(data = abdat, 
                       prop = .7)

# extract training and testing data
ab_train <- training(split)
ab_test <- testing(split)
```

### Fit a ridge regression model

-   ***Question 2***. Use the model.matrix() function to create a
    predictor matrix, x, and assign the Rings variable to an outcome
    vector, y.

```{r}
# create a model matrix X from our training data
X <- model.matrix(Rings ~ ., ab_train)[ ,-1]

# set our outcome vector Y from our training data
Y <- ab_train$Rings
```

-   ***Question 3***. Fit a ridge model (controlled by the alpha
    parameter) using the glmnet() function. Make a plot showing how the
    estimated coefficients change with lambda. (Hint: You can call
    plot() directly on the glmnet() objects).

```{r}
# fit a ridge model 
ridge_ab <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# plot the ridge model estimate coefficients
plot(ridge_ab, xvar = "lambda")
```

### Using *k*-fold cross validation resampling and tuning our models

In lecture we learned about two methods of estimating our model's
generalization error by resampling, cross validation and bootstrapping.
We'll use the *k*-fold cross validation method in this lab. Recall that
lambda is a tuning parameter that helps keep our model from over-fitting
to the training data. Tuning is the process of finding the optimal value
of lambda.

-   ***Question 4***. This time fit a ridge regression model and a lasso
    model, both with using cross validation. The glmnet package kindly
    provides a cv.glmnet() function to do this (similar to the glmnet()
    function that we just used). Use the alpha argument to control which
    type of model you are running. Plot the results.

```{r}
# ridge regression model using k-folds CV
ridge_ab_cv <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# lasso model using k-folds CV
lasso_ab_cv <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)

# plot results
par(mfrow = c(1, 2))
plot(ridge_ab_cv, main = "Ridge penalty\n\n")
plot(lasso_ab_cv, main = "Lasso penalty\n\n")
```

-   ***Question 5***. Interpret the graphs. What is being displayed on
    the axes here? How does the performance of the models change with
    the value of lambda?

-   These graphs display the relationship between changing values of
    lambda (x-axis) and the mean-squared error of our model. The values
    at the top of the graph refer to the number of features retained in
    the models, which only changes for lasso as the model automatically
    removes less influential features. With our ridge penalty, the
    smallest values of lambda give us the lowest mean-squared error.
    With our lasso penalty, we have a wide 1 standard error range.
    Within these ranges, we would choose the model that gives us the
    optimal tradeoff between number of features (lasso) or lowest
    coefficients (ridge) and mean-squared error. For both models, as
    lambda increases, our model performance declines.

-   ***Question 6***. Inspect the ridge model object you created with
    cv.glmnet(). The \$cvm column shows the MSEs for each CV fold. What
    is the minimum MSE? What is the value of lambda associated with this
    MSE minimum?

```{r}
# ridge model minimum MSE value
paste("ridge penalty minimum MSE:", round(min(ridge_ab_cv$cvm), 2), "rings")

# lambda value at this min MSE
paste("ridge penalty lambda value at the min MSE:", round(ridge_ab_cv$lambda.min, 4))
```

-   ***Question 7***. Do the same for the lasso model. What is the
    minimum MSE? What is the value of lambda associated with this MSE
    minimum?

```{r}
# lasso model minimum MSE value
paste("lasso penalty minimum MSE:", round(min(lasso_ab_cv$cvm), 2), "rings")

# lambda value at this min MSE
paste("lasso penalty lambda value at the min MSE:", round(lasso_ab_cv$lambda.min, 4))
```

Data scientists often use the "one-standard-error" rule when tuning
lambda to select the best model. This rule tells us to pick the most
parsimonious model (fewest number of predictors) while still remaining
within one standard error of the overall minimum cross validation error.
The cv.glmnet() model object has a column that automatically finds the
value of lambda associated with the model that produces an MSE that is
one standard error from the MSE minimum (\$lambda.1se).

-   ***Question 8***. Find the number of predictors associated with this
    model (hint: the \$nzero is the \# of predictors column).

```{r}
## find the number of predictors at the lambda value 1 standard error from the MSE
# lasso model
paste("number of predictors at the lambda value 1 SE from the MSE for lasso penalty:", lasso_ab_cv$nzero[lasso_ab_cv$lambda == lasso_ab_cv$lambda.1se])

# ridge model
paste("number of predictors at the lambda value 1 SE from the MSE for ridge penalty:", ridge_ab_cv$nzero[ridge_ab_cv$lambda == ridge_ab_cv$lambda.1se])
```

-   ***Question 9***. Which regularized regression worked better for
    this task, ridge or lasso? Explain your answer.

```{r}
# lasso penalty lambda value at the MSE 1 SE from the min MSE
paste("lasso penalty lambda value at the MSE 1-SE:", round(lasso_ab_cv$cvm[lasso_ab_cv$lambda == lasso_ab_cv$lambda.1se], 2))

# ridge penalty lambda value at the MSE 1 SE from the min MSE
paste("ridge penalty lambda value at the MSE 1-SE:", round(ridge_ab_cv$cvm[ridge_ab_cv$lambda == ridge_ab_cv$lambda.1se], 2))
```

-   The lasso penalty model appears to work better for this task because
    it offers a lower MSE at the point 1 standard error from the minimum
    MSE than the ridge penalty (4.99 vs 5.35), while also reducing the
    number of predictors from 10 to 6. With lasso penalty, we've
    achieved a more parsimonious model with fewer predictors and a lower
    MSE.
