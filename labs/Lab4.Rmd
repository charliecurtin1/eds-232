---
title: "Lab4"
author: "Charlie Curtin"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(skimr)
library(tidymodels)
library(caret)
library(corrplot)
library(broom)
library(knitr)
library(kableExtra)
library(ggpubr)
library(gridExtra)
library(patchwork)
library(ConfusionTableR)
```

## Lab 4: Fire and Tree Mortality

The database we'll be working with today includes 36066 observations of individual trees involved in prescribed fires and wildfires occurring over 35 years, from 1981 to 2016. It is a subset of a larger fire and tree mortality database from the US Forest Service (see data description for the full database here: [link](https://www.nature.com/articles/s41597-020-0522-7#Sec10)). Our goal today is to predict the likelihood of tree mortality after a fire.

### Data Exploration

Outcome variable: *yr1status* = tree status (0=alive, 1=dead) assessed one year post-fire.

Predictors: *YrFireName, Species, Genus_species, DBH_cm, CVS_percent, BCHM_m, BTL* (Information on these variables available in the database metadata ([link](https://www.fs.usda.gov/rds/archive/products/RDS-2020-0001-2/_metadata_RDS-2020-0001-2.html))).

```{r, include = FALSE}
# read in data
trees_dat <- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/trees-dat.csv")

# view the structure of the data
str(trees_dat)
```

> Question 1: Recode all the predictors to a zero_based integer form

```{r}
## preprocessing ------
# drop the ID column
trees_dat <- trees_dat %>% 
  select(-"...1")

# specify a recipe to with yr1status as our target variable and all others as predictors
trees_dat_rec <- recipe(yr1status ~ ., data = trees_dat) %>% 
  # convert all predictors to integer form
  step_integer(all_string(), zero_based = TRUE) %>% 
  prep(trees_dat)

# bake our recipe
trees_dat_bake <- bake(trees_dat_rec, new_data = trees_dat)
```


### Data Splitting

> Question 2: Create trees_training (70%) and trees_test (30%) splits for the modeling

```{r}
# split the baked data
trees_split <- initial_split(trees_dat_bake, prop = .7)

# extract the training and test data
trees_train <- training(trees_split)
trees_test <- testing(trees_split)
```

> Question 3: How many observations are we using for training with this split?

- There are 25246 observations in the training set.

### Simple Logistic Regression 

Let's start our modeling effort with some simple models: one predictor and one outcome each.

> Question 4: Choose the three predictors that most highly correlate with our outcome variable for further investigation.

```{r}
# create a correlation matrix
m_trees <- cor(trees_train)
corrplot(m_trees, method = c("number"), type = "upper")
```
- The three predictors most highly correlated with yr1status are CVS_percent, BCHM_m, and DBH_cm

> Question 5: Use glm() to fit three simple logistic regression models, one for each of the predictors you identified.

```{r}
# fit three different models with CVS_percent, BCHM_m, and DBH_cm -----
model_cvs <- glm(data = trees_train, yr1status ~ CVS_percent, family = "binomial")

model_bchm <- glm(data = trees_train, yr1status ~ BCHM_m, family = "binomial")

model_dbh <- glm(data = trees_train, yr1status ~ DBH_cm, family = "binomial")


# exponentiate coefficients -----
cvs_coef <- exp(coef(model_cvs))

bchm_coef <- exp(coef(model_bchm))

dbh_coef <- exp(coef(model_dbh))


# show results in a table -----
# create a dataframe of coefficients
exp_coef_df <- rbind(
  data.frame(model = "CVS_percent", intercept = cvs_coef[1], 
             B1 = cvs_coef[2]),
  data.frame(model = "BCHM_m", intercept = bchm_coef[1], 
             B1 = bchm_coef[2]),
  data.frame(model = "DBH_cm", intercept = dbh_coef[1], 
             B1 = dbh_coef[2])
)


# format results into a table
glm_coef <- kable(exp_coef_df, format = "html", row.names = FALSE,
                  caption = "Odds ratios for GLM models predicting tree mortality") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

glm_coef
```


### Interpret the Coefficients 

We aren't always interested in or able to interpret the model coefficients in a machine learning task. Often predictive accuracy is all we care about.

> Question 6: That said, take a stab at interpreting our model coefficients now.

- For CVS_percent, the intercept means that when CVS_percent is 0, the odds of a tree being alive increase multiplicatively by .001. The B1 coefficient means that when CVS_percent increases by 1 percentage point, the odds of a tree being alive increase multiplicatively by 1.08. For BCHM_m, the intercept means that when BCHM_m is 0, the odds of a tree being alive increase multiplicatively by .15. The B1 coefficient means that for each 1 meter increase in BCHM, the odds of a tree being alive increase multiplicatively by 1.23. For DBH_cm, the intercept means that when DBH_cm is 0, the odds of a tree being alive increase multiplicatively by 1.67. The B1 coefficient means that for each 1 cm increase in DBH, the odds of a tree being alive increase multiplicatively by .94. 


> Question 7: Now let's visualize the results from these models. Plot the fit to the training data of each model.

```{r}
# CVS percent
ggplot(trees_train, aes(x = CVS_percent, y = yr1status)) +
  geom_point() +
  stat_smooth(method = "glm", se = TRUE, method.args = list(family = binomial))

# DBH_cm
ggplot(trees_train, aes(x = DBH_cm, y = yr1status)) +
  geom_point() +
  stat_smooth(method = "glm", se = TRUE, method.args = list(family = binomial))

# BCHM_m
ggplot(trees_train, aes(x = BCHM_m, y = yr1status)) +
  geom_point() +
  stat_smooth(method = "glm", se = TRUE, method.args = list(family = binomial))
```


### Multiple Logistic Regression

Let's not limit ourselves to a single-predictor model. More predictors might lead to better model performance.

> Question 8: Use glm() to fit a multiple logistic regression called "logistic_full", with all three of the predictors included. Which of these are significant in the resulting model?

```{r}
# fit a multiple logistic regression on yr1status using our 3 predictors
logistic_full <- glm(data = trees_train, yr1status ~ CVS_percent + BCHM_m + DBH_cm, family = "binomial")

summary(logistic_full)
```
- All three predictors are signicant in our model with a p-value less than .001.

### Estimate Model Accuracy

Now we want to estimate our model's generalizability using resampling.

> Question 9: Use cross validation to assess model accuracy. Use caret::train() to fit four 10-fold cross-validated models (cv_model1, cv_model2, cv_model3, cv_model4) that correspond to each of the four models we've fit so far: three simple logistic regression models corresponding to each of the three key predictors (CVS_percent, DBH_cm, BCHM_m) and a multiple logistic regression model that combines all three predictors.

```{r}
# convert our outcome variable to a factor
trees_train <- trees_train %>% 
  mutate(yr1status = as.factor(yr1status))

trees_test <- trees_test %>% 
  mutate(yr1status = as.factor(yr1status))

## estimate model generalizability
# specify K-folds cv and the number of folds
control <- trainControl(method = "cv", number = 10)

## cross-validate our 4 different models
# CVS
cv_model1 <- caret::train(yr1status ~ CVS_percent, data = trees_train, method = "glm",
                           family = "binomial", trControl = control)

# BCHM
cv_model2 <- caret::train(yr1status ~ BCHM_m, data = trees_train, method = "glm",
                           family = "binomial", trControl = control)

# DBH
cv_model3 <- caret::train(yr1status ~ DBH_cm, data = trees_train, method = "glm",
                           family = "binomial", trControl = control)

# all 3 predictors
cv_model4 <- caret::train(yr1status ~ CVS_percent + BCHM_m + DBH_cm, 
                          data = trees_train, method = "glm",
                          family = "binomial", trControl = control)
```

> Question 10: Use caret::resamples() to extract then compare the classification accuracy for each model. (Hint: resamples() wont give you what you need unless you convert the outcome variable to factor form). Which model has the highest accuracy?

```{r}
# use resamples to assess classification accuracy on all 4 of our models
resamps <- caret::resamples(list(cv_model1, cv_model2, cv_model3, cv_model4))

summary(resamps)
```
- Our model with all 3 predictors has the highest maximum accuracy at .92. It also has the highest Kappa statistic with a max of .8, which takes into account correct classification occurring by chance.

Let's move forward with this single most accurate model.

> Question 11: Compute the confusion matrix and overall fraction of correct predictions by the model.

```{r}
# fit our 3 predictors model to our training data
model4_train <- predict(cv_model4, newdata = trees_train, type = "raw")

# bind predictions and observed values 
predictions_train <- cbind(data.frame(predictions = model4_train, 
                                      true = trees_train$yr1status))

# compute confusion matrix
cm_train <- caret::confusionMatrix(predictions_train$predictions, predictions_train$true)

# show confusion table
ConfusionTableR::binary_visualiseR(train_labels = predictions_train$predictions,
                                   truth_labels= predictions_train$true,
                                   class_label1 = "Not Dead", 
                                   class_label2 = "Dead",
                                   quadrant_col1 = "#28ACB4", 
                                   quadrant_col2 = "#4397D2", 
                                   custom_title = "Tree Mortality Confusion Matrix (train)", 
                                   text_col= "black")
```

> Question 12: Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

- The confusion matrix tell us the number of true positives (predicted not dead, tree not dead), false positives (predicted not dead, tree dead), true negatives (predicted dead, tree dead), and false negatives (predicted dead, tree not dead) predicted by our model. The mistakes are false negatives and false positives.

> Question 13: What is the overall accuracy of the model? How is this calculated?

- The overall accuracy of our model is 90.3%. This is calculated by dividing the sum of true positive and true negatives by the number of total observations in our training set

### Test Final Model

Alright, now we'll take our most accurate model and make predictions on some unseen data (the test data).

> Question 14: Now that we have identified our best model, evaluate it by running a prediction on the test data, trees_test.

```{r}
# fit our 3 predictors model to our testing data
model4_test <- predict(cv_model4, newdata = trees_test, type = "raw")

# bind predictions and observed values 
predictions_test <- cbind(data.frame(predictions = model4_test, 
                                     true = trees_test$yr1status))

# compute confusion matrix
cm_test <- caret::confusionMatrix(predictions_test$predictions, predictions_test$true)

# show confusion table
ConfusionTableR::binary_visualiseR(train_labels = predictions_test$predictions,
                                   truth_labels= predictions_test$true,
                                   class_label1 = "Not Dead", 
                                   class_label2 = "Dead",
                                   quadrant_col1 = "#28ACB4", 
                                   quadrant_col2 = "#4397D2", 
                                   custom_title = "Tree Mortality Confusion Matrix (test)", 
                                   text_col= "black")
```

> Question 15: How does the accuracy of this final model on the test data compare to its cross validation accuracy? Do you find this to be surprising? Why or why not?

- The accuracy on the final model is 90.6%, very similar to the cross-validation accuracy of 90.3%. It's not surprising 
