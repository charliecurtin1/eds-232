---
title: "Lab 8"
author: "Charlie Curtin"
date: "2024-03-06"
output: html_document
---

## Forest Cover Classification with SVM

In this week's lab we are exploring the use of Support Vector Machines for multi-class classification. Specifically, you will be using cartographic variables to predict forest cover type (7 types).

Natural resource managers responsible for developing ecosystem management strategies require basic descriptive information including inventory data like forest cover type for forested lands to support their decision-making processes. However, managers generally do not have this type of data for in-holdings or neighboring lands that are outside their immediate jurisdiction. One method of obtaining this information is through the use of predictive models.

You task is build both an SVM and a random forest model and compare their performance on accuracy and computation time.

1.  The data is available here: <https://ucsb.box.com/s/ai5ost029enlguqyyn04bnlfaqmp8kn4>

```{r, warning = FALSE, message = FALSE, include = FALSE}
# load required packages
library(tidymodels)
library(tidyverse)
library(kernlab)
library(here)
library(doParallel)
```

```{r, warning = FALSE, message = FALSE}
set.seed(435)

# read in forest cover data
forest <- read_csv(here("labs", "data", "covtype_sample.csv")) %>% 
  # convert our outcome variable to a factor
  mutate(Cover_Type = as.factor(Cover_Type))
```

Explore the data.

```{r, eval = FALSE}
str(forest)

ggplot(data = forest, aes(x = Cover_Type)) +
  geom_bar()
```
-   What kinds of features are we working with?
  
  We have a number of continuous variables describing various topological attributes of different patches of forest. We also have a few one-hot encoded categorical variables for soil type and whether the patch is in a particular National Forest. Our outcome variable is a categorical outcome with 7 different forest cover types.

-   Does anything stand out that will affect you modeling choices?

  When we plot a histogram of our outcome variable, we see that cover type is right-skewed, with many more observations falling into the first two forest cover types than the others. This indicates that we might want to make some sort of transformation, potentially a log transformation, when we specify our recipe.

2.  Create the recipe and carry out any necessary preprocessing. Can you use the same recipe for both models?
  We can use the same recipe for both models, with the only necessary preprocessing being normalizing our numeric predictors.

```{r, message = FALSE, warning = FALSE}
# split into training and testing data
forest_split <- initial_split(forest)

forest_train <- training(forest_split)

forest_test <- testing(forest_split)

# specify a recipe for our models
rec <- recipe(Cover_Type ~ ., data = forest_train) %>% 
  # exclude our binary variables from preprocessing because they have no variance
  step_zv() %>% 
  # normalize our numeric predictors
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())

# create folds for cross-validation
folds <- vfold_cv(forest_train, strata = Cover_Type)
```


## Support Vector Machine

```{r, warning = FALSE, message = FALSE}
## tune a support vector machine model
# define a radial basis function svm model
svm_spec <- svm_rbf(cost = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")

# bundle our model and recipe into a workflow
svm_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(svm_spec)

# 
svm_rbf_wf <- workflow() %>%
  add_model(svm_spec %>%
  set_args(cost_tune())) %>%
  add_formula(Cover_Type ~ .)
```

```{r, warning = FALSE, message = FALSE}
# parallel processing to reduce computation time
doParallel::registerDoParallel()

# tune hyperparameter values
grid <- grid_regular(cost(), levels = 5)

system.time(svm_tune <- tune_grid(
  svm_rbf_wf,
  resamples = folds,
  grid = grid
))

# visualize results
autoplot(svm_tune)
```

```{r, message = FALSE, warning = FALSE}
## finalize a workflow and fit our model to our test data
# choose our best svm model with cost tune based on accuracy
best_svm <- select_best(svm_tune, metric = "accuracy")

# finalize workflow with our best model
svm_rbf_final <- finalize_workflow(svm_rbf_wf, best_svm)

# fit our workflow to our training data
svm_rbf_fit <- svm_rbf_final %>% 
  fit(forest_train)
```

```{r, message = FALSE, warning = FALSE}
# make predictions on our new data and view the model performance in a confusion matrix
augment(svm_rbf_fit, new_data = forest_test) %>%
  conf_mat(truth = Cover_Type, estimate = .pred_class)
```

```{r}
# view our final model's accuracy
svm_accuracy <- augment(svm_rbf_fit, new_data = forest_test) %>%
  accuracy(truth = Cover_Type, estimate = .pred_class)

print(paste("Our support vector machine model's prediction accuracy:", svm_accuracy$.estimate))
```


## Random Forest

```{r, message = FALSE, warning = FALSE}
## tuning a random forest model
# define a random forest model
rf_spec <- rand_forest(mtry = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# create a workflow with our recipe
rf_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(rf_spec)
```

```{r, warning = FALSE, message = FALSE}
# build trees in parallel
doParallel::registerDoParallel()

# use k-folds cv to tune our model
system.time(
  rf_tune <- rf_wf %>% 
  tune_grid(
    resamples = folds,
    metrics = metric_set(accuracy),
    grid = 5
  ))
```

```{r, message = FALSE, warning = FALSE}
# select our best random forest model
rf_final <- finalize_workflow(rf_wf, select_best(rf_tune))

# fit best model to our data
rf_best_fit <- fit(rf_final, forest_train)

# make predictions on our new data and view the model performance in a confusion matrix
augment(rf_best_fit, new_data = forest_test) %>%
  conf_mat(truth = Cover_Type, estimate = .pred_class)
```

```{r}
# fit predictions on our testing data
rf_preds <- predict(rf_best_fit, forest_test) %>% 
  bind_cols(forest_test)

# assess accuracy of our random forest model
rf_accuracy <- accuracy(rf_preds, truth = Cover_Type,
         estimate = .pred_class)

print(paste("Our random forest model's prediction accuracy:", rf_accuracy$.estimate))
```

-   Which type of model do you think is better for this task?

  Based on computational costs, our support vector machine model is better for this task, taking around 9 minutes. The random forest model took 13 minutes to run. However, the random forest model had a marginally higher accuracy, at .78 vs .76.

-   Why do you speculate this is the case?
