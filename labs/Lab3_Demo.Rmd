---
title: "Lab 3 Demo"
author: "Charlie Curtin"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample) # to sample and split our dataset
library(skimr) # exploratory package for datasets
library(glmnet)
```

## Data Wrangling and Exploration
```{r data}
# load and inspect the data
dat <- AmesHousing::make_ames()

```

##Train a model
```{r intial_split}
# Data splitting with {rsample} 
set.seed(123) #set a seed for reproducibility

# split data- defaults to 75/25
split <- initial_split(dat)

# extract the training and test sets
ames_train <- training(split)
ames_test  <- testing(split)
```

```{r model_data}
# Create training feature matrices using model.matrix() (auto encoding of categorical variables)
# specify an object- formula and a dataset
# use the brackets at the end to remove the intercept, which is all 1s and generated by the model matrix
X <- model.matrix(Sale_Price ~ ., ames_train)[,-1]

# transform y with log() transformation
Y <- log(ames_train$Sale_Price)

# run skim() in the console to view summaries of the dataset

```

```{r glmnet}
# fit a ridge model, passing X,Y,alpha to glmnet()
## alpha sets the model type (ridge, lasso, elasticnet)
ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# plot() the glmnet model object to view lambda
plot(ridge, xvar = "lambda")  
```

```{r}
# lambdas applied to penalty parameter.  Examine the first few
ridge$lambda %>% 
  head()

# small lambda results in large coefficients, looking at the 100th row (small lambda)
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 100]

# what about for small coefficients? looking at the 1st row (large lambda)
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 1]
  
```
How much improvement to our loss function as lambda changes?

##Tuning
```{r cv.glmnet}
# Apply CV ridge regression to Ames data.  Same arguments as before to glmnet()
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Apply CV lasso regression to Ames data
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)
  
# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```
- x axis: log lambda
- y axis: root mean squared error (want to minimize)
- left dashed line: which value of lambda gives optimal mean squared error
- right dashed line: one standard error rule/one standard deviation rule. Want to pick the point of lambda that gives you the best combination of features in your model and prediction (parsimony = least number of features but highest prediction power). So essentially there's a tradeoff between MSE and number of features. 
- x axis up top: number of features in the model (lasso is performing feature selection as the lambdas change)
- grey lines around the red lines: with k-folds we're doing 10 iterations, and the plot is the average of those predictions, so it's the variation amongst all those predictions.


10-fold CV MSE for a ridge and lasso model. What's the "rule of 1 standard error"?

In both models we see a slight improvement in the MSE as our penalty log(Î») gets larger, suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to increase. 

Let's examine the important parameter values apparent in the plots.
```{r}
# Ridge model
# minimum MSE value
min(ridge$cvm)

# lambda value at this min MSE
ridge$lambda.min

# MSE with the 1 standard error rule
ridge$cvm[ridge$lambda == ridge$lambda.1se]

# lambda for this MSE
ridge$lambda.1se

# Lasso model
# minimum MSE
min(lasso$cvm)

# lambda for this min MSE
lasso$lambda.min

# MSE with the 1 standard error rule
lasso$cvm[lasso$lambda == lasso$lambda.1se]

# lambda for this MSE
lasso$lambda.1se

# No. of coef | 1-SE MSE
lasso$nzero[lasso$lambda == lasso$lambda.1se]

# No. of coef | min MSE
lasso$nzero[lasso$lambda == lasso$lambda.min]
```
- bottom number is the number of features left. From the minimum MSE to the 1-SE MSE, we've gone from 119 to 29 features


```{r}
# Ridge model
ridge_min 

# Lasso model
lasso_min


par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```

